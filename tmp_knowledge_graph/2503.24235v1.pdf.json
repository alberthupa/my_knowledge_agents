{
  "nodes": {
    "Light-R1": {
      "attributes": {}
    },
    "JMLE-2024": {
      "attributes": {}
    },
    "aggregation": {
      "attributes": {}
    },
    "Self-Repetition Strategy": {
      "attributes": {
        "description": "Stimulates the LLM to generate multiple samples instead of individual ones by prompting the LLM repeatedly during the decoding stage."
      }
    },
    "math": {
      "attributes": {}
    },
    "Scaling Curves (Accuracy vs. Compute)": {
      "attributes": {
        "definition": "Visualizations of how metrics such as accuracy, pass rate, or EM improve as token budgets, iteration depth, or the number of samples increase."
      }
    },
    "Bradley-Terry model": {
      "attributes": {}
    },
    "LongBench": {
      "attributes": {}
    },
    "DeepScaler": {
      "attributes": {}
    },
    "OREO": {
      "attributes": {}
    },
    "C-Eval": {
      "attributes": {}
    },
    "SimPO": {
      "attributes": {}
    },
    "Miverva": {
      "attributes": {}
    },
    "k-\u03f5Controllability": {
      "attributes": {}
    },
    "human cognition": {
      "attributes": {}
    },
    "problem-solving capabilities": {
      "attributes": {}
    },
    "GSM8K": {
      "attributes": {}
    },
    "```": {
      "attributes": {}
    },
    "AlpacaEval2.0": {
      "attributes": {}
    },
    "decode strategy": {
      "attributes": {}
    },
    "Kaoyan": {
      "attributes": {}
    },
    "Fusion": {
      "attributes": {
        "description": "Approaches that merge multiple samples into one to solve the problem of low sample quality of candidates.",
        "relation": "Involves merging multiple samples into one.",
        "example": "Replacing the majority voting in self-consistency with generative self-aggregation (Li et al., 2025k)."
      }
    },
    "reasoning tasks": {
      "attributes": {}
    },
    "pretraining": {
      "attributes": {}
    },
    "HumanEval-Mul": {
      "attributes": {}
    },
    "C-SimpleQA": {
      "attributes": {}
    },
    "CHAIR": {
      "attributes": {}
    },
    "PoT": {
      "attributes": {}
    },
    "Stimulation": {
      "attributes": {
        "description": "Encourages the model to generate longer or multiple candidate outputs.",
        "category": "Inference-based Approach",
        "part_of": "How to Scale"
      }
    },
    "internal scaling": {
      "attributes": {}
    },
    "SciBench": {
      "attributes": {}
    },
    "PRIME": {
      "attributes": {
        "description": "Integrates the SFT model as a PRM within a unified RL framework, allowing online updates through policy rollouts and outcome labels via implicit process rewards."
      }
    },
    "Mean Deviation from Target Length": {
      "attributes": {
        "definition": "Quantifies the average relative difference between the generated output length and the target length.",
        "formula": "Mean Deviation = Ex\u223cD [|ngenerated \u2212 ngold| / ngold]",
        "ngenerated": "The model\u2019s output length.",
        "ngold": "The target length."
      }
    },
    "Pass@k": {
      "attributes": {
        "description": "Extends Pass@1 by measuring whether at least one of the model\u2019s k sampled outputs is correct."
      }
    },
    "parallel scaling": {
      "attributes": {}
    },
    "application scenarios": {
      "attributes": {}
    },
    "algorithmic mechanisms": {
      "attributes": {}
    },
    "Scaling": {
      "attributes": {
        "description": "Scaling test-time reasoning in large language models through logic unit alignment",
        "type": "Method"
      }
    },
    "artificial general intelligence": {
      "attributes": {}
    },
    "verification": {
      "attributes": {}
    },
    "task domains": {
      "attributes": {}
    },
    "CodeMind": {
      "attributes": {}
    },
    "prompt strategy": {
      "attributes": {}
    },
    "Cons@k": {
      "attributes": {}
    },
    "RFTT": {
      "attributes": {}
    },
    "CVBench": {
      "attributes": {}
    },
    "DVPO": {
      "attributes": {
        "description": "Presents a streamlined framework, substituting the reward model with a pre-trained global value model and removing the dependency between the actor and critic."
      }
    },
    "outcome": {
      "attributes": {}
    },
    "Self-Evaluation": {
      "attributes": {}
    },
    "JAMA Clinical Challenge": {
      "attributes": {}
    },
    "NuminaMath": {
      "attributes": {}
    },
    "task performance": {
      "attributes": {}
    },
    "TAU-bench": {
      "attributes": {}
    },
    "WebV oyager": {
      "attributes": {}
    },
    "This text is about various benchmarks and techniques used in the field of large language models (LLMs), particularly focusing on test-time scaling. It covers different categories of benchmarks like code, science, games, medicine, general-purpose, agents, knowledge, open-ended tasks, and multi-modal tasks. It also discusses methods for how to scale, including tuning-based approaches like supervised finetuning (SFT) and reinforcement learning (RL), and inference-based approaches. The text also mentions parallel, sequential and hybrid scaling.": {
      "attributes": {}
    },
    "FRAMES": {
      "attributes": {}
    },
    "FLOPs-based Efficiency Analysis": {
      "attributes": {
        "definition": "A method to quantify computational cost using FLOPs (Floating Point Operations per Second)."
      }
    },
    "UGDA": {
      "attributes": {
        "description": "Leverages the uncertainty and influence of samples during PPO training and iteratively refines the reward model."
      }
    },
    "taxonomy": {
      "attributes": {}
    },
    "OpenR": {
      "attributes": {}
    },
    "pipeline": {
      "attributes": {}
    },
    "JEEBench": {
      "attributes": {}
    },
    "ArenaHard": {
      "attributes": {}
    },
    "performance dimensions": {
      "attributes": {}
    },
    "CMMLU": {
      "attributes": {
        "description": "Measuring massive multitask language understanding in Chinese",
        "type": "Benchmark"
      }
    },
    "CPL": {
      "attributes": {}
    },
    "OlympiadBench": {
      "attributes": {}
    },
    "HLE": {
      "attributes": {}
    },
    "real-world effectiveness": {
      "attributes": {}
    },
    "Long Chain-of-Thought": {
      "attributes": {}
    },
    "Selective DPO": {
      "attributes": {}
    },
    "GPQA": {
      "attributes": {}
    },
    "Open-Reasoner-Zero": {
      "attributes": {}
    },
    "Naive PRM": {
      "attributes": {}
    },
    "mixture-of-model": {
      "attributes": {}
    },
    "Tree-of-Thoughts (ToT)": {
      "attributes": {
        "description": "Generalizes CoT to a branching search. At each reasoning step, the model can generate multiple candidate thoughts instead of one, forming a tree of possibilities. It evaluates these candidates and selects the most promising branch(es) to continue expanding.",
        "properties": "Iterative, structured inference without additional training, can be modeled as a search process through a state space of partial solutions."
      }
    },
    "WinRate": {
      "attributes": {}
    },
    "Self-Reflection Feedback": {
      "attributes": {}
    },
    "AReaL": {
      "attributes": {}
    },
    "Deductive PRM": {
      "attributes": {}
    },
    "Science OlympicArena": {
      "attributes": {}
    },
    "LLA V A-Wild": {
      "attributes": {}
    },
    "OmniMath": {
      "attributes": {}
    },
    "Codeforces": {
      "attributes": {}
    },
    "Heuristic Function": {
      "attributes": {}
    },
    "SciEval": {
      "attributes": {}
    },
    "The most important entities (key concepts) in it are test-time scaling, inference efficiency, compute analysis, controllability, scalability, and the various metrics used to evaluate these aspects.": {
      "attributes": {}
    },
    "Scaling Metric": {
      "attributes": {
        "definition": "Captures the average slope of performance gains as compute increases.",
        "formula": "Scaling = (1/|A|^2) * \u03a3 [f(b) \u2212 f(a)] / (b \u2212 a)",
        "A": "Set of compute values",
        "f(x)": "Performance at compute value x"
      }
    },
    "DQO": {
      "attributes": {}
    },
    "Rest-MCTS*": {
      "attributes": {
        "description": "Uses tree-search-based RL to bypass per-step manual annotation typically required for training process rewards."
      }
    },
    "Multi-Agent Verifiers": {
      "attributes": {
        "description": "Scaling test-time compute with goal verifiers",
        "type": "Verification method"
      }
    },
    "ARC-AGI": {
      "attributes": {}
    },
    "MATH-Vision": {
      "attributes": {}
    },
    "Token Cost": {
      "attributes": {}
    },
    "SimpleRL": {
      "attributes": {}
    },
    "open challenges": {
      "attributes": {}
    },
    "New Entities:": {
      "attributes": {}
    },
    "Forest-of-Thought": {
      "attributes": {}
    },
    "developmental trajectories": {
      "attributes": {}
    },
    "Length-filtered vote": {
      "attributes": {}
    },
    "MM-Vet": {
      "attributes": {}
    },
    "stimulation": {
      "attributes": {}
    },
    "Generative Verifier": {
      "attributes": {}
    },
    "DAPO": {
      "attributes": {}
    },
    "Throughput": {
      "attributes": {
        "definition": "A measure of the amount of data or requests processed per unit of time, often critical in real-world applications, especially for high-throughput systems."
      }
    },
    "Breadth-First Search (BFS)": {
      "attributes": {
        "description": "A search strategy that expands all plausible thoughts at each depth, keeping the top b best states based on f(s)."
      }
    },
    "cDPO": {
      "attributes": {}
    },
    "Logic-RL": {
      "attributes": {
        "description": "Unleashing LLM reasoning with rule-based reinforcement learning"
      }
    },
    "MMBench": {
      "attributes": {}
    },
    "OpenRLHF": {
      "attributes": {}
    },
    "decomposition": {
      "attributes": {}
    },
    "This text is a survey paper about test-time scaling (TTS) in large language models (LLMs). It proposes a unified framework to analyze TTS methods based on four core dimensions: what to scale, how to scale, where to scale, and how well to scale. The paper reviews existing methods, application scenarios, and assessment aspects, and identifies open challenges and future research directions in TTS.": {
      "attributes": {}
    },
    "ReST-MCTS": {
      "attributes": {}
    },
    "Mixture-of-Model Strategy": {
      "attributes": {
        "description": "Gathers the \u201cwisdom of the crowd\u201d by coordinated sampling across multiple models."
      }
    },
    "Control Metric": {
      "attributes": {
        "definition": "A formal metric to quantify adherence to a specified compute budget range.",
        "formula": "Control = (1/|A|) * \u03a3 I(amin \u2264 a \u2264 amax)",
        "A": "The set of observed compute values such as thinking tokens.",
        "I(\u00b7)": "The indicator function."
      }
    },
    "WoT": {
      "attributes": {}
    },
    "REINFORCE++": {
      "attributes": {
        "description": "Simplifies GRPO and enhances its training."
      }
    },
    "k\u2013\u03f5 controllability": {
      "attributes": {}
    },
    "Depth-First Search (DFS)": {
      "attributes": {
        "description": "A search strategy that follows the most promising thought path deeply, backtracking if necessary."
      }
    },
    "Underthinking score": {
      "attributes": {
        "definition": "Quantifies the inefficiency of a model when it initially generates a correct thought but fails to follow through to a correct final answer.",
        "formula": "\u03beUT = (1/N) * \u03a3(1 - (\u02c6Ti/Ti))",
        "N": "Number of incorrect responses in the test set.",
        "Ti": "Total number of tokens in the i-th incorrect response.",
        "\u02c6Ti": "Number of tokens from the beginning of the response up to and including the first correct thought."
      }
    },
    "input modification": {
      "attributes": {}
    },
    "Graph-of-Thoughts": {
      "attributes": {}
    },
    "selection": {
      "attributes": {}
    },
    "GPQA Diamond": {
      "attributes": {}
    },
    "distillation": {
      "attributes": {}
    },
    "MathVista": {
      "attributes": {}
    },
    "RLOO": {
      "attributes": {}
    },
    "```text": {
      "attributes": {}
    },
    "Aider-Polyglot": {
      "attributes": {}
    },
    "SWE-bench Lite": {
      "attributes": {}
    },
    "AGIEval": {
      "attributes": {
        "description": "A human-centric benchmark for evaluating foundation models",
        "category": "Benchmark"
      }
    },
    "search": {
      "attributes": {}
    },
    "BCFL": {
      "attributes": {}
    },
    "MMStar": {
      "attributes": {}
    },
    "State Evaluation": {
      "attributes": {
        "description": "The process of estimating the quality of a partial state s using an evaluation function f(s).",
        "properties": "Function f: S -> R, may be implemented by the model itself using a self-evaluation prompt or a scoring heuristic."
      }
    },
    "test-time scaling": {
      "attributes": {}
    },
    "Length Deviation": {
      "attributes": {}
    },
    "reasoning": {
      "attributes": {}
    },
    "open-ended Q&A": {
      "attributes": {}
    },
    "reasoning models": {
      "attributes": {}
    },
    "output verification": {
      "attributes": {}
    },
    "Outcome Verification": {
      "attributes": {
        "description": "Techniques employed at test time in LLMs, operating on the fly during inference, often by generating multiple solutions and using a proposer\u2013verifier framework.",
        "category": "Technique"
      }
    },
    "USACO": {
      "attributes": {}
    },
    "Codeforces Percentile": {
      "attributes": {}
    },
    "rStar-Math": {
      "attributes": {}
    },
    "Medbullets": {
      "attributes": {}
    },
    "TextCraft": {
      "attributes": {}
    },
    "Selection": {
      "attributes": {
        "description": "Selecting samples from one single LM, or selecting samples generated by weak and strong LLMs as a routing problem.",
        "example": "Agentic (Parmar et al., 2025) agent considering both current and previous status"
      }
    },
    "coding": {
      "attributes": {}
    },
    "LCPO": {
      "attributes": {
        "description": "Focuses on optimizing accuracy and adherence to user-specified length constraints for reasoning tasks."
      }
    },
    "supervised finetuning": {
      "attributes": {}
    },
    "sequential scaling": {
      "attributes": {}
    },
    "multidimensional framework": {
      "attributes": {}
    },
    "Elo Rating": {
      "attributes": {}
    },
    "SPPD": {
      "attributes": {
        "description": "Utilizes process preference learning with a dynamic value margin for self-training."
      }
    },
    "VC-PPO": {
      "attributes": {}
    },
    "Pass@1": {
      "attributes": {
        "description": "A metric that measures the proportion of problems where the model\u2019s first generated solution is correct.",
        "usage": "Frequently used in tasks such as mathematical reasoning and coding benchmarks."
      }
    },
    "OpenR1": {
      "attributes": {}
    },
    "OVM": {
      "attributes": {}
    },
    "code": {
      "attributes": {}
    },
    "Gaokao": {
      "attributes": {}
    },
    "assessment aspects": {
      "attributes": {}
    },
    "Focused-DPO": {
      "attributes": {}
    },
    "hybrid scaling": {
      "attributes": {}
    },
    "IF-Eval": {
      "attributes": {}
    },
    "CodeAlphaCode": {
      "attributes": {}
    },
    "general tasks": {
      "attributes": {}
    },
    "MR-Ben": {
      "attributes": {}
    },
    "inference": {
      "attributes": {}
    },
    "mathematics": {
      "attributes": {}
    },
    "scaling laws": {
      "attributes": {}
    },
    "XoT": {
      "attributes": {}
    },
    "The text is about test-time scaling (TTS) in large language models (LLMs), focusing on inference efficiency metrics, compute analysis, and controllability. It discusses various metrics for evaluating TTS methods, including latency, throughput, underthinking score, KV cache size, control metrics, length deviation metrics, and scalability metrics. It also provides a practical guideline for TTS deployment and highlights challenges and opportunities in the field.": {
      "attributes": {}
    },
    "TravelPlan": {
      "attributes": {}
    },
    "SimpleQA": {
      "attributes": {}
    },
    "Latency": {
      "attributes": {
        "definition": "A measure of the time delay in a system, often critical in real-world applications, especially for high-throughput systems."
      }
    },
    "STaR": {
      "attributes": {}
    },
    "SciWorld": {
      "attributes": {}
    },
    "Algorithm-of-Thought": {
      "attributes": {}
    },
    "WebShop": {
      "attributes": {}
    },
    "REINFORCE": {
      "attributes": {}
    },
    "Chatbot Arena": {
      "attributes": {}
    },
    "self-repetition": {
      "attributes": {}
    },
    "MedQA": {
      "attributes": {}
    },
    "V-STaR": {
      "attributes": {}
    },
    "Search": {
      "attributes": {
        "description": "Systematically explores the sample space.",
        "category": "Inference-based Approach",
        "part_of": "How to Scale"
      }
    },
    "tuning": {
      "attributes": {}
    },
    "SimpleRL-Zoo": {
      "attributes": {}
    },
    "System 2 AI": {
      "attributes": {
        "description": "A survey of reasoning large language models",
        "type": "Survey"
      }
    },
    "Points24": {
      "attributes": {}
    },
    "State Verifier": {
      "attributes": {}
    },
    "deployment": {
      "attributes": {}
    },
    "RMSE of Length Deviation": {
      "attributes": {
        "definition": "Captures the variance in length control.",
        "formula": "RMSE = sqrt((1/N) * \u03a3[(ngenerated,i \u2212 ngold,i) / ngold,i]^2)",
        "ngenerated,i": "The model\u2019s output length for the i-th sample.",
        "ngold,i": "The target length for the i-th sample.",
        "N": "Number of samples."
      }
    },
    "process state": {
      "attributes": {}
    },
    "fusion": {
      "attributes": {}
    },
    "WebArena": {
      "attributes": {}
    },
    "FollowBench": {
      "attributes": {}
    },
    "reinforcement learning": {
      "attributes": {}
    },
    "s1": {
      "attributes": {}
    },
    "Naive ORM": {
      "attributes": {}
    },
    "Unit Test": {
      "attributes": {}
    },
    "TinyZero": {
      "attributes": {}
    },
    "MMLU-Pro": {
      "attributes": {}
    },
    "GRPO": {
      "attributes": {
        "description": "Replaces traditional value models with improved sampling strategies, accelerates learning, and achieves performance comparable to GPT-4 in mathematics."
      }
    },
    "VinePPO": {
      "attributes": {
        "description": "Unlocking RL potential for LLM reasoning through refined credit assignment",
        "type": "Reinforcement Learning method"
      }
    },
    "FrontierMath": {
      "attributes": {}
    },
    "KV Cachesize": {
      "attributes": {}
    },
    "SysBench": {
      "attributes": {
        "category": "Game & Strategy"
      }
    },
    "Monte Carlo Tree Search (MCTS)": {
      "attributes": {
        "description": "An approach that integrates into generative model inference for problem-solving, concentrating simulations on the most promising branches. Value estimates converge to optimal values in certain perfect-information games."
      }
    },
    "ReMax": {
      "attributes": {
        "description": "A simple, effective, and efficient reinforcement learning method for aligning large language models",
        "type": "Reinforcement Learning method"
      }
    },
    "CodeContests": {
      "attributes": {}
    },
    "Self-Refine": {
      "attributes": {
        "category": "Representative Method",
        "description": "An advanced TTS technique that enables an LLM to iteratively improve its own outputs through self-generated feedback. It involves initial output generation, feedback generation, and refinement steps. Requires no additional training data or fine-tuning.",
        "properties": "Uses a feedback-refinement loop, leverages test-time compute, enhances model reliability."
      }
    },
    "SWE-bench": {
      "attributes": {}
    },
    "Thought Generation": {
      "attributes": {
        "description": "The process of generating a set of next-step thoughts given the current state (context).",
        "properties": "Function G(s) -> {t1, t2, ..., tb}, where each ti represents a candidate next reasoning step."
      }
    },
    "Verification": {
      "attributes": {
        "description": "Filters or scores outputs based on correctness or other criteria.",
        "category": "Inference-based Approach",
        "part_of": "How to Scale"
      }
    },
    "MMMU": {
      "attributes": {}
    },
    "inference time": {
      "attributes": {}
    },
    "X-R1": {
      "attributes": {}
    },
    "Process Verification": {
      "attributes": {
        "description": "Verifies the sample outcomes and the process of obtaining such an outcome, commonly adopted in tasks with formal, deductive processes."
      }
    },
    "TheoremQA": {
      "attributes": {}
    },
    "Best-of-N": {
      "attributes": {
        "description": "A method where the best output among N generated samples is selected.",
        "relation": "Extended by Brown et al. (2024b) and Li et al. (2023a) to weigh each sample by its score from external verifiers.",
        "abbreviation": "BoN",
        "category": "Representative Method"
      }
    },
    "NVIDIA H100 GPUs": {
      "attributes": {
        "type": "Hardware",
        "description": "Used for faster inference when KV cache sharing is promoted."
      }
    }
  },
  "edges": [
    {
      "source": "REINFORCE++",
      "relation": "simplifies and enhances",
      "target": "GRPO"
    },
    {
      "source": "Self-Repetition Strategy",
      "relation": "is a type of",
      "target": "Stimulation"
    },
    {
      "source": "Mixture-of-Model Strategy",
      "relation": "is a type of",
      "target": "Stimulation"
    },
    {
      "source": "Pass@k",
      "relation": "extends",
      "target": "Pass@1"
    },
    {
      "source": "Thought Generation",
      "relation": "is a step in",
      "target": "Tree-of-Thoughts (ToT)"
    },
    {
      "source": "State Evaluation",
      "relation": "is a step in",
      "target": "Tree-of-Thoughts (ToT)"
    },
    {
      "source": "Tree-of-Thoughts (ToT)",
      "relation": "can employ",
      "target": "Breadth-First Search (BFS)"
    },
    {
      "source": "Tree-of-Thoughts (ToT)",
      "relation": "can employ",
      "target": "Depth-First Search (DFS)"
    }
  ]
}