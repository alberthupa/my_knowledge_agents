{
  "nodes": {
    "* initial batch": {
      "attributes": {}
    },
    "knowledge deficiencies": {
      "attributes": {
        "description": "Deficiencies in models discovered by SEA on a massive knowledge base.",
        "properties": "Identified by SEA. Can be related to specific categories of knowledge (e.g., culture and the arts, history and events)."
      }
    },
    "* Russian Localities and Districts": {
      "attributes": {}
    },
    "* free-response format": {
      "attributes": {}
    },
    "* clustering algorithm": {
      "attributes": {}
    },
    "misinformation": {
      "attributes": {}
    },
    "foundation model": {
      "attributes": {}
    },
    "Sentence Transformers": {
      "attributes": {
        "description": "Used to pre-embed page titles and abstracts for efficient retrieval",
        "example": "mGTE (Zhang et al., 2024c)"
      }
    },
    "latent factual encoding": {
      "attributes": {}
    },
    "* question rephrasing": {
      "attributes": {}
    },
    "* memory-context conflict": {
      "attributes": {}
    },
    "factual information": {
      "attributes": {}
    },
    "* retrieved information": {
      "attributes": {}
    },
    "* error rate": {
      "attributes": {}
    },
    "* memory": {
      "attributes": {}
    },
    "error-related retrieval": {
      "attributes": {}
    },
    "closed-weight models": {
      "attributes": {
        "description": "Models with weights that are not updated during inference."
      }
    },
    "* Metro/Railway/Transportation Projects": {
      "attributes": {}
    },
    "multiple-choice question-answer pairs": {
      "attributes": {}
    },
    "document level": {
      "attributes": {}
    },
    "hierarchical retrieval": {
      "attributes": {}
    },
    "probing agents": {
      "attributes": {}
    },
    "* source pruning": {
      "attributes": {}
    },
    "* Baseball": {
      "attributes": {}
    },
    "error propagation": {
      "attributes": {}
    },
    "* skill": {
      "attributes": {}
    },
    "error-based subset updates": {
      "attributes": {}
    },
    "semantic similarity": {
      "attributes": {}
    },
    "* vector quantization": {
      "attributes": {}
    },
    "* reasoning models": {
      "attributes": {}
    },
    "* source error": {
      "attributes": {}
    },
    "* Deutsches Hygiene-Museum, Dresden": {
      "attributes": {}
    },
    "error-related documents": {
      "attributes": {}
    },
    "* markers": {
      "attributes": {}
    },
    "* Whitney Biennial": {
      "attributes": {}
    },
    "Stochastic Error Ascent (SEA)": {
      "attributes": {
        "definition": "A scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget.",
        "properties": "Surpasses previous baselines (ACD and AutoBencher) by uncovering more errors at lower cost per error. Achieves a 100% human pass rate on generated questions, exposes distinct error clusters across models, and delivers critical insights for enhancing model reliability.",
        "description": "A process that identifies knowledge deficiencies in language models.",
        "procedures": "(1) collecting and updating the source error set (line 9 in Alg. 1) and (2) directed graph construction and source pruning (lines 10 and 11 in Alg. 1)",
        "key characteristics": [
          "scalable and efficient framework",
          "formulates error discovery as a stochastic optimization process",
          "iteratively retrieves new high-error candidates based on semantic similarity to previously observed failures",
          "enhances search efficiency and coverage with hierarchical retrieval across document and paragraph levels",
          "uses a relation directed acyclic graph (relation DAG)"
        ],
        "performance": [
          "uncovers 40.7\u00d7 more knowledge errors than Automated Capability Discovery",
          "uncovers 26.7% more than AutoBencher",
          "reduces cost-per-error by 599\u00d7 compared to ACD",
          "reduces cost-per-error by 9\u00d7 compared to AutoBencher"
        ],
        "comparison": [
          "Automated Capability Discovery",
          "AutoBencher"
        ],
        "evaluation": "Surpasses previous baselines by discovering more errors at lower cost."
      }
    },
    "AutoBencher": {
      "attributes": {
        "properties": "A baseline for comparison with SEA.",
        "description": "A baseline method for comparison with SEA.",
        "method": "Takes an input topic and retrieves related pages iteratively from a given knowledge base to build a challenging benchmark",
        "generator_model": "gpt-4o",
        "interest_topics": "All categories",
        "comparison performance": [
          "26.7% fewer knowledge errors uncovered compared to SEA",
          "9\u00d7 higher cost-per-error compared to SEA"
        ],
        "takes_input_topic_from": "Wikipedia categories",
        "generates_benchmarks": "13",
        "type": "evaluation tool"
      }
    },
    "* testee model": {
      "attributes": {}
    },
    "* testee prompt": {
      "attributes": {}
    },
    "* deficiencies": {
      "attributes": {}
    },
    "* Simple Time/Date/Day Mentions": {
      "attributes": {}
    },
    "fine-tuning": {
      "attributes": {}
    },
    "multi-agent systems": {
      "attributes": {}
    },
    "optimization": {
      "attributes": {}
    },
    "* context": {
      "attributes": {}
    },
    "* categories": {
      "attributes": {}
    },
    "* Coaching, Teams, and Seasons": {
      "attributes": {}
    },
    "knowledge-intensive benchmarks": {
      "attributes": {}
    },
    "* internal activations": {
      "attributes": {}
    },
    "embedding dimension": {
      "attributes": {
        "used_for": "efficient retrieval",
        "technique": "FAISS"
      }
    },
    "* shape": {
      "attributes": {}
    },
    "* color": {
      "attributes": {}
    },
    "relation directed acyclic graph": {
      "attributes": {}
    },
    "* gerund phrase": {
      "attributes": {}
    },
    "* non-reasoning models": {
      "attributes": {}
    },
    "relation directed acyclic graph (relation DAG)": {
      "attributes": {}
    },
    "* oil on cardboard": {
      "attributes": {}
    },
    "New Entities:": {
      "attributes": {}
    },
    "* Kunstmuseum Bern": {
      "attributes": {}
    },
    "error discovery": {
      "attributes": {
        "research_on": "limited for self-discovering misinformation in LLMs",
        "methods": [
          "ACD",
          "AutoBencher"
        ]
      }
    },
    "* Kunst Raum Riehen": {
      "attributes": {}
    },
    "* College Football Polls & All American Lists": {
      "attributes": {}
    },
    "None": {
      "attributes": {}
    },
    "* Music": {
      "attributes": {}
    },
    "English-based knowledge base": {
      "attributes": {}
    },
    "Wikipedia": {
      "attributes": {
        "properties": "Used as the initial set for searching errors. Thirteen categories of Wikipedia are used.",
        "description": "A knowledge base.",
        "size": "7.1M documents and 28.8M paragraphs",
        "structure": "Organized into 13 top-level categories with hierarchical subcategories",
        "paragraph_extraction": "Sections in the document are mapped as paragraphs",
        "preprocessing": "Each page has a preprocessed abstract",
        "documents": "7.1M",
        "paragraphs": "28.8M",
        "categories": "13 top-level categories",
        "category": "knowledge source"
      }
    },
    "factual knowledge modeling": {
      "attributes": {}
    },
    "* art exhibition": {
      "attributes": {}
    },
    "* external-augmented information": {
      "attributes": {}
    },
    "* error pattern": {
      "attributes": {}
    },
    "stochastic error ascent": {
      "attributes": {}
    },
    "vulnerabilities": {
      "attributes": {}
    },
    "* human pass rate": {
      "attributes": {}
    },
    "question generator": {
      "attributes": {}
    },
    "stochastic optimization process": {
      "attributes": {
        "description": "An optimization process that involves randomness."
      }
    },
    "gradient-guided prompt generation": {
      "attributes": {}
    },
    "* topic": {
      "attributes": {}
    },
    "* Soccer / Association Football": {
      "attributes": {}
    },
    "* Swimming and Paralympic/Olympic Sports": {
      "attributes": {}
    },
    "* markdown": {
      "attributes": {}
    },
    "* random initial batch": {
      "attributes": {}
    },
    "* category constraints": {
      "attributes": {}
    },
    "* accuracy": {
      "attributes": {}
    },
    "* topic constraint": {
      "attributes": {}
    },
    "* decoding parameters": {
      "attributes": {}
    },
    "* VITRINE": {
      "attributes": {}
    },
    "knowledge errors": {
      "attributes": {}
    },
    "This text is about evaluating large language models (LLMs) by identifying their failure patterns and vulnerabilities, particularly in knowledge-intensive tasks. It describes a methodology for searching for errors, analyzing error patterns, and using prompts to generate questions and rephrase them. The text also discusses using BERT models to identify paragraphs that can trigger errors in LLMs.": {
      "attributes": {}
    },
    "failure modes": {
      "attributes": {}
    },
    "hierarchical capability trees": {
      "attributes": {
        "description": "Used to explain and organize the capabilities of models."
      }
    },
    "dynamic benchmarking": {
      "attributes": {}
    },
    "* LDA": {
      "attributes": {}
    },
    "* pre-trained parametric knowledge": {
      "attributes": {}
    },
    "* inverted file system": {
      "attributes": {}
    },
    "* ground truth answer": {
      "attributes": {}
    },
    "source errors": {
      "attributes": {}
    },
    "knowledge base": {
      "attributes": {}
    },
    "systematic prompt design": {
      "attributes": {}
    },
    "cumulative error": {
      "attributes": {
        "description": "Error that accumulates over steps in a process.",
        "symbol": "TS( fclose)",
        "properties": "Low-quality sources not pruned by cumulative error can negatively affect SEA."
      }
    },
    "* testee models": {
      "attributes": {}
    },
    "data coverage": {
      "attributes": {}
    },
    "This text is a list of authors and their affiliations, likely from a research paper or technical report. It also includes references to various research papers and URLs. The main topic seems to be related to large language models, their evaluation, and trustworthiness.": {
      "attributes": {}
    },
    "* American Football": {
      "attributes": {}
    },
    "Massive Knowledge Base": {
      "attributes": {
        "properties": "Used to discover knowledge deficiencies of language models.",
        "category": "information storage system"
      }
    },
    "Large Language Models (LLMs)": {
      "attributes": {
        "definition": "Language models that possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs.",
        "properties": "Can have knowledge deficiencies that SEA aims to discover. Can be closed-weight models.",
        "description": "Language models that are evaluated for their failure patterns and vulnerabilities, particularly in knowledge-intensive tasks.",
        "examples": "gpt-4o, gpt-4o-mini, o1-mini, DeepSeek-V3, DeepSeek-R1, R1-Distill-Llama-70B, Qwen2.5-72B-Instruct, Llama-3.3-70B-Instruct",
        "capabilities": "impressive linguistic capabilities",
        "deficiencies": [
          "often fail to faithfully retain factual knowledge",
          "leading to hallucinations",
          "unreliable outputs"
        ],
        "knowledge sources": "pretrained on vast corpora, including comprehensive knowledge sources such as Wikipedia",
        "use cases": "healthcare, law, scientific research",
        "issues": "fail to retain or accurately reproduce factual information, resulting in misinformation"
      }
    },
    "query budget": {
      "attributes": {}
    },
    "* unknown knowledge": {
      "attributes": {}
    },
    "* error distribution": {
      "attributes": {}
    },
    "Automated Capability Discovery": {
      "attributes": {
        "properties": "A baseline for comparison with SEA.",
        "description": "A baseline method for comparison with SEA.",
        "acronym": "ACD",
        "method": "Leverages LLM\u2019s internal knowledge to self-discover successes and failures",
        "generator_model": "gpt-4o",
        "type": "evaluation tool"
      }
    },
    "* random selection": {
      "attributes": {}
    },
    "* per-step error": {
      "attributes": {}
    },
    "hallucinations": {
      "attributes": {}
    },
    "* art space": {
      "attributes": {}
    },
    "* multiple choice question generation": {
      "attributes": {}
    },
    "data leakage": {
      "attributes": {}
    },
    "errors": {
      "attributes": {}
    },
    "* error tasks": {
      "attributes": {}
    },
    "model behavior understanding": {
      "attributes": {}
    },
    "* API calls": {
      "attributes": {}
    },
    "* QA set": {
      "attributes": {}
    },
    "* skill description": {
      "attributes": {}
    },
    "paragraph level": {
      "attributes": {}
    },
    "* cross-validation": {
      "attributes": {}
    },
    "* oil on canvas": {
      "attributes": {}
    },
    "* ablation experiments": {
      "attributes": {}
    }
  },
  "edges": [
    {
      "source": "Large Language Models (LLMs)",
      "relation": "are pretrained on",
      "target": "Wikipedia"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "is compared to",
      "target": "Automated Capability Discovery"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "is compared to",
      "target": "AutoBencher"
    },
    {
      "source": "Automated Capability Discovery",
      "relation": "compared with",
      "target": "Stochastic Error Ascent (SEA)"
    },
    {
      "source": "AutoBencher",
      "relation": "compared with",
      "target": "Stochastic Error Ascent (SEA)"
    },
    {
      "source": "cumulative error",
      "relation": "tracked during",
      "target": "Stochastic Error Ascent (SEA)"
    },
    {
      "source": "knowledge deficiencies",
      "relation": "found by",
      "target": "Stochastic Error Ascent (SEA)"
    },
    {
      "source": "Wikipedia",
      "relation": "used as",
      "target": "knowledge base"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "discovers",
      "target": "knowledge deficiencies"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "surpasses",
      "target": "Automated Capability Discovery"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "surpasses",
      "target": "AutoBencher"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "identifies",
      "target": "knowledge deficiencies"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "frames",
      "target": "stochastic optimization process"
    },
    {
      "source": "Large Language Models (LLMs)",
      "relation": "evaluated for",
      "target": "vulnerabilities"
    },
    {
      "source": "Large Language Models (LLMs)",
      "relation": "can have",
      "target": "knowledge deficiencies"
    },
    {
      "source": "knowledge deficiencies",
      "relation": "identified by",
      "target": "Stochastic Error Ascent (SEA)"
    },
    {
      "source": "This text is a list of authors and their affiliations, likely from a research paper or technical report. It also includes references to various research papers and URLs. The main topic seems to be related to large language models, their evaluation, and trustworthiness.",
      "relation": "mentions",
      "target": "Large Language Models (LLMs)"
    },
    {
      "source": "Large Language Models (LLMs)",
      "relation": "can exhibit",
      "target": "knowledge deficiencies"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "compared to",
      "target": "AutoBencher"
    },
    {
      "source": "Large Language Models (LLMs)",
      "relation": "pretrained on",
      "target": "Wikipedia"
    },
    {
      "source": "error discovery",
      "relation": "methods",
      "target": "AutoBencher"
    },
    {
      "source": "Stochastic Error Ascent (SEA)",
      "relation": "evaluates",
      "target": "closed-weight models"
    },
    {
      "source": "Massive Knowledge Base",
      "relation": "contains",
      "target": "factual information"
    }
  ]
}